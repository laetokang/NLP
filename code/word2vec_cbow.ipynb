{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    b = np.random.randn(output_size, 1) * 0.01\n",
    "    return W, b\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W, b = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    parameters['b'] = b\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds -- shape: (CBOW_N, number of examples)\n",
    "    \"\"\"\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vecs = np.take(WRD_EMB, inds.T, axis=0).T\n",
    "    \n",
    "    assert(word_vecs.shape == (WRD_EMB.shape[1], inds.shape[0], inds.shape[1]))\n",
    "    \n",
    "    return word_vecs\n",
    "\n",
    "def mean_(word_vecs):\n",
    "    word_vecs_mean = np.mean(word_vecs, axis=1)\n",
    "    word_vecs_mean = word_vecs_mean.reshape(word_vecs.shape[0], -1)\n",
    "    \n",
    "    assert(word_vecs_mean.shape == (word_vecs.shape[0], word_vecs.shape[2]))\n",
    "    \n",
    "    return word_vecs_mean\n",
    "\n",
    "def linear_dense(word_vecs_mean, parameters):\n",
    "    W, b = parameters['W'], parameters['b']\n",
    "    Z = np.dot(W, word_vecs_mean) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], word_vecs_mean.shape[1]))\n",
    "    \n",
    "    return W, b, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vecs = ind_to_word_vecs(inds, parameters)\n",
    "    word_vecs_mean = mean_(word_vecs)\n",
    "    W, b, Z = linear_dense(word_vecs_mean, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vecs'] = word_vecs\n",
    "    caches['word_vecs_mean'] = word_vecs_mean\n",
    "    caches['W'] = W\n",
    "    caches['b'] = b\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, caches):\n",
    "    Z = caches['Z']\n",
    "    dL_dZ = Z - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == Z.shape)\n",
    "    \n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    Z = W * X + b\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    b = caches['b']\n",
    "    word_vecs_mean = caches['word_vecs_mean']\n",
    "    m = word_vecs_mean.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vecs_mean.T)\n",
    "    dL_db = (1 / m) * np.sum(dL_dZ, axis=1, keepdims=True)\n",
    "    dL_dword_vecs_mean = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(b.shape == dL_db.shape)\n",
    "    assert(word_vecs_mean.shape == dL_dword_vecs_mean.shape)\n",
    "    \n",
    "    return dL_dW, dL_db, dL_dword_vecs_mean\n",
    "\n",
    "def mean_backward(dL_dword_vecs_mean, caches):\n",
    "    \"\"\"\n",
    "    ele_mean_i = (1 / n) * (ele_1 + ele_2 + ele_n)\n",
    "    \"\"\"\n",
    "    word_vecs = caches['word_vecs']\n",
    "    CBOW_N = word_vecs.shape[1]\n",
    "    \n",
    "#     dL_dword_vecs = (1 / m) * (1 / CBOW_N) * np.ones((dL_dword_vecs_mean.shape[0], CBOW_N)) *\\\n",
    "#         np.sum(dL_dword_vecs_mean, axis=1, keepdims=True)\n",
    "        \n",
    "    dL_dword_vecs = (1 / m) * (1 / CBOW_N) * np.sum(dL_dword_vecs_mean, axis=1, keepdims=True)\n",
    "    assert((word_vecs.shape[0], 1) == dL_dword_vecs.shape)\n",
    "    \n",
    "    return dL_dword_vecs\n",
    "\n",
    "def backward_propagation(Y, caches):\n",
    "    dL_dZ = softmax_backward(Y, caches)\n",
    "    dL_dW, dL_db, dL_dword_vecs_mean = dense_backward(dL_dZ, caches)\n",
    "    dL_dword_vecs = mean_backward(dL_dword_vecs_mean, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_db'] = dL_db\n",
    "    gradients['dL_dword_vecs'] = dL_dword_vecs\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    CBOW_N = caches['inds'].shape[0]\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    dL_dword_vecs = gradients['dL_dword_vecs']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    \n",
    "#     updated_WRD_EMD = parameters['WRD_EMB'][inds.T, :] -\\\n",
    "#         learning_rate * gradients['dL_dword_vecs'].T.reshape(1, CBOW_N, -1)\n",
    "    for i in range(m):\n",
    "        WRD_EMB[inds[:, i], :] -= dL_dword_vecs.T * learning_rate\n",
    "    \n",
    "#     parameters['WRD_EMB'][inds.flatten(), :] = updated_WRD_EMD.reshape(-1, emb_size)\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']\n",
    "    parameters['b'] -= learning_rate * gradients['dL_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "import gc\n",
    "\n",
    "\n",
    "def cbow_model(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "\n",
    "#     batch_inds = list(range(0, m, batch_size))\n",
    "    for epoch in range(epochs):\n",
    "#         np.random.shuffle(batch_inds)\n",
    "        for b, i in enumerate(range(0, m, batch_size)):\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "#             if b % 100 == 0:\n",
    "#                 print('epoch {}, {}/{} - Cost: {}'.format(epoch, b, 1000, np.squeeze(cost)))\n",
    "#             gc.collect()\n",
    "#             del X_batch\n",
    "#             del Y_batch\n",
    "#             del caches\n",
    "#             del gradients\n",
    "            \n",
    "            \n",
    "        costs.append(cost)\n",
    "        if print_cost and epoch % 1000 == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, np.squeeze(cost)))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Data : the quick brown fox jumped over the lazy dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_toy = ['the','quick','brown','fox','jumped','over','the','lazy','dog']\n",
    "id_to_word_toy = dict()\n",
    "word_to_id_toy = dict()\n",
    "i = 0\n",
    "for token in text_toy:\n",
    "    if token in word_to_id_toy:\n",
    "        continue\n",
    "    id_to_word_toy[i] = token\n",
    "    word_to_id_toy[token] = i\n",
    "    i += 1\n",
    "window_size = 1\n",
    "example_len = 2 * 1 + 1\n",
    "X_toy = []\n",
    "Y_toy = []\n",
    "\n",
    "for i in range(len(text_toy) - example_len + 1):\n",
    "    X_toy.extend([word_to_id_toy[word] for word in text_toy[i:i+1] + text_toy[i+2:i+3]])\n",
    "    Y_toy.append(word_to_id_toy[text_toy[i+1]])\n",
    "        \n",
    "X_toy = np.array(X_toy)\n",
    "X_toy = X_toy.reshape(-1, window_size * 2).T\n",
    "Y_toy = np.array(Y_toy)\n",
    "Y_toy = Y_toy.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8\n",
    "m = X_toy.shape[-1]\n",
    "emb_size = 6\n",
    "\n",
    "Y_toy_one_hot = np.zeros((vocab_size, m))\n",
    "Y_toy_one_hot[Y_toy.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.071669643729546\n",
      "Cost after epoch 1000: 2.0561158641183903\n",
      "Cost after epoch 2000: 2.054868062259784\n",
      "Cost after epoch 3000: 2.0547102072415426\n",
      "Cost after epoch 4000: 2.0546409362318405\n",
      "Cost after epoch 5000: 2.0545789115204616\n",
      "Cost after epoch 6000: 2.0545174883779818\n",
      "Cost after epoch 7000: 2.0544561238145422\n",
      "Cost after epoch 8000: 2.054394773562\n",
      "Cost after epoch 9000: 2.0543334341167845\n",
      "Cost after epoch 10000: 2.0542721053063957\n",
      "Cost after epoch 11000: 2.054210787229377\n",
      "Cost after epoch 12000: 2.0541494800050644\n",
      "Cost after epoch 13000: 2.0540881837530143\n",
      "Cost after epoch 14000: 2.054026898591212\n",
      "Cost after epoch 15000: 2.053965624635827\n",
      "Cost after epoch 16000: 2.053904362001086\n",
      "Cost after epoch 17000: 2.0538431107991713\n",
      "Cost after epoch 18000: 2.053781871140115\n",
      "Cost after epoch 19000: 2.0537206431317077\n",
      "Cost after epoch 20000: 2.053659426879404\n",
      "Cost after epoch 21000: 2.0535982224862375\n",
      "Cost after epoch 22000: 2.053537030052741\n",
      "Cost after epoch 23000: 2.053475849676867\n",
      "Cost after epoch 24000: 2.0534146814539227\n",
      "Cost after epoch 25000: 2.053353525476499\n",
      "Cost after epoch 26000: 2.0532923818344178\n",
      "Cost after epoch 27000: 2.053231250614671\n",
      "Cost after epoch 28000: 2.053170131901378\n",
      "Cost after epoch 29000: 2.05310902577574\n",
      "Cost after epoch 30000: 2.0530479323160016\n",
      "Cost after epoch 31000: 2.052986851597422\n",
      "Cost after epoch 32000: 2.052925783692246\n",
      "Cost after epoch 33000: 2.052864728669683\n",
      "Cost after epoch 34000: 2.0528036865958916\n",
      "Cost after epoch 35000: 2.052742657533969\n",
      "Cost after epoch 36000: 2.0526816415439457\n",
      "Cost after epoch 37000: 2.0526206386827828\n",
      "Cost after epoch 38000: 2.0525596490043774\n",
      "Cost after epoch 39000: 2.05249867255957\n",
      "Cost after epoch 40000: 2.0524377093961617\n",
      "Cost after epoch 41000: 2.0523767595589235\n",
      "Cost after epoch 42000: 2.052315823089627\n",
      "Cost after epoch 43000: 2.052254900027063\n",
      "Cost after epoch 44000: 2.0521939904070767\n",
      "Cost after epoch 45000: 2.052133094262595\n",
      "Cost after epoch 46000: 2.052072211623667\n",
      "Cost after epoch 47000: 2.052011342517503\n",
      "Cost after epoch 48000: 2.051950486968517\n",
      "Cost after epoch 49000: 2.0518896449983717\n",
      "Cost after epoch 50000: 2.0518288166260272\n",
      "Cost after epoch 51000: 2.051768001867795\n",
      "Cost after epoch 52000: 2.0517072007373836\n",
      "Cost after epoch 53000: 2.0516464132459635\n",
      "Cost after epoch 54000: 2.0515856394022167\n",
      "Cost after epoch 55000: 2.0515248792123995\n",
      "Cost after epoch 56000: 2.0514641326804015\n",
      "Cost after epoch 57000: 2.0514033998078083\n",
      "Cost after epoch 58000: 2.051342680593963\n",
      "Cost after epoch 59000: 2.0512819750360336\n",
      "Cost after epoch 60000: 2.0512212831290735\n",
      "Cost after epoch 61000: 2.0511606048660895\n",
      "Cost after epoch 62000: 2.0510999402381094\n",
      "Cost after epoch 63000: 2.0510392892342453\n",
      "Cost after epoch 64000: 2.050978651841763\n",
      "Cost after epoch 65000: 2.050918028046147\n",
      "Cost after epoch 66000: 2.0508574178311685\n",
      "Cost after epoch 67000: 2.0507968211789533\n",
      "Cost after epoch 68000: 2.0507362380700456\n",
      "Cost after epoch 69000: 2.0506756684834753\n",
      "Cost after epoch 70000: 2.0506151123968253\n",
      "Cost after epoch 71000: 2.0505545697862946\n",
      "Cost after epoch 72000: 2.050494040626763\n",
      "Cost after epoch 73000: 2.050433524891855\n",
      "Cost after epoch 74000: 2.0503730225540036\n",
      "Cost after epoch 75000: 2.0503125335845103\n",
      "Cost after epoch 76000: 2.050252057953608\n",
      "Cost after epoch 77000: 2.0501915956305203\n",
      "Cost after epoch 78000: 2.050131146583519\n",
      "Cost after epoch 79000: 2.0500707107799836\n",
      "Cost after epoch 80000: 2.0500102881864573\n",
      "Cost after epoch 81000: 2.049949878768702\n",
      "Cost after epoch 82000: 2.049889482491752\n",
      "Cost after epoch 83000: 2.049829099319968\n",
      "Cost after epoch 84000: 2.0497687292170865\n",
      "Cost after epoch 85000: 2.049708372146272\n",
      "Cost after epoch 86000: 2.049648028070163\n",
      "Cost after epoch 87000: 2.0495876969509235\n",
      "Cost after epoch 88000: 2.049527378750284\n",
      "Cost after epoch 89000: 2.0494670734295894\n",
      "Cost after epoch 90000: 2.0494067809498415\n",
      "Cost after epoch 91000: 2.049346501271739\n",
      "Cost after epoch 92000: 2.0492862343557205\n",
      "Cost after epoch 93000: 2.0492259801620007\n",
      "Cost after epoch 94000: 2.04916573865061\n",
      "Cost after epoch 95000: 2.049105509781431\n",
      "Cost after epoch 96000: 2.049045293514229\n",
      "Cost after epoch 97000: 2.048985089808692\n",
      "Cost after epoch 98000: 2.048924898624458\n",
      "Cost after epoch 99000: 2.048864719921148\n"
     ]
    }
   ],
   "source": [
    "parameters = cbow_model(X_toy, Y_toy_one_hot, vocab_size, emb_size, 0.0025, 100000, parameters=None, batch_size=126, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRD_EMB = parameters['WRD_EMB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_similar(word, wrd_emb, n=10):\n",
    "    id_ = word_to_id_toy[word]\n",
    "    vec_word = wrd_emb[id_, :]\n",
    "    norm_vec_word = np.linalg.norm(vec_word)\n",
    "    cos_sim = np.dot(wrd_emb, vec_word.T) / (np.linalg.norm(wrd_emb, axis=1) * norm_vec_word)\n",
    "    top_n_ind = np.argsort(cos_sim)[-n:][::-1]\n",
    "    return [id_to_word_toy[id_] for id_ in top_n_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lazy', 'brown', 'dog', 'the', 'quick', 'jumped', 'over', 'fox']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_top_n_similar('lazy', WRD_EMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- https://github.com/ujhuyz0110/wrd_emb/blob/master/word2vec_cbow.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
