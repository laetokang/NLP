{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\"\n",
    "tokens = tokenize(doc)\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    initialize all the trianing parameters\n",
    "    \"\"\"\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    WRD_EMB[inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=True, plot_cost=True):\n",
    "    \"\"\"\n",
    "    X: Input word indices. shape: (1, m)\n",
    "    Y: One-hot encodeing of output word indices. shape: (vocab_size, m)\n",
    "    vocab_size: vocabulary size of your corpus or training data\n",
    "    emb_size: word embedding size. How many dimensions to represent each vocabulary\n",
    "    learning_rate: alaph in the weight update formula\n",
    "    epochs: how many epochs to train the model\n",
    "    batch_size: size of mini batch\n",
    "    parameters: pre-trained or pre-initialized parameters\n",
    "    print_cost: whether or not to print costs during the training process\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "    \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.5520873161230386\n",
      "Cost after epoch 10: 2.5517584893578444\n",
      "Cost after epoch 20: 2.5514074826182243\n",
      "Cost after epoch 30: 2.5510128404839265\n",
      "Cost after epoch 40: 2.550552512371465\n",
      "Cost after epoch 50: 2.550002597039341\n",
      "Cost after epoch 60: 2.5493498518737026\n",
      "Cost after epoch 70: 2.548559145576941\n",
      "Cost after epoch 80: 2.547598050631854\n",
      "Cost after epoch 90: 2.546431217743277\n",
      "Cost after epoch 100: 2.545018541020228\n",
      "Cost after epoch 110: 2.543347973157434\n",
      "Cost after epoch 120: 2.541352332312607\n",
      "Cost after epoch 130: 2.538974515501255\n",
      "Cost after epoch 140: 2.536154710690859\n",
      "Cost after epoch 150: 2.53282764106512\n",
      "Cost after epoch 160: 2.528999044499819\n",
      "Cost after epoch 170: 2.524554868659921\n",
      "Cost after epoch 180: 2.5194196056178315\n",
      "Cost after epoch 190: 2.5135287925689234\n",
      "Cost after epoch 200: 2.5068271526222587\n",
      "Cost after epoch 210: 2.499418759432453\n",
      "Cost after epoch 220: 2.491194038082997\n",
      "Cost after epoch 230: 2.4821558586232486\n",
      "Cost after epoch 240: 2.4723608003372735\n",
      "Cost after epoch 250: 2.461911731598491\n",
      "Cost after epoch 260: 2.4511612670443075\n",
      "Cost after epoch 270: 2.4401376248444633\n",
      "Cost after epoch 280: 2.4290397244386455\n",
      "Cost after epoch 290: 2.4181040817459554\n",
      "Cost after epoch 300: 2.407559727494446\n",
      "Cost after epoch 310: 2.3977750533994375\n",
      "Cost after epoch 320: 2.3887036378771125\n",
      "Cost after epoch 330: 2.380385098433377\n",
      "Cost after epoch 340: 2.3728062773818275\n",
      "Cost after epoch 350: 2.365887254463288\n",
      "Cost after epoch 360: 2.359608088001814\n",
      "Cost after epoch 370: 2.353695798973024\n",
      "Cost after epoch 380: 2.34797197735971\n",
      "Cost after epoch 390: 2.3422747302108164\n",
      "Cost after epoch 400: 2.3364561551183716\n",
      "Cost after epoch 410: 2.330501154098893\n",
      "Cost after epoch 420: 2.324222227187406\n",
      "Cost after epoch 430: 2.317534599222066\n",
      "Cost after epoch 440: 2.3103892022772783\n",
      "Cost after epoch 450: 2.3027600039936953\n",
      "Cost after epoch 460: 2.2947930434722044\n",
      "Cost after epoch 470: 2.286387588553747\n",
      "Cost after epoch 480: 2.277559233444492\n",
      "Cost after epoch 490: 2.268354210236898\n",
      "Cost after epoch 500: 2.2588316753380666\n",
      "Cost after epoch 510: 2.249239412572052\n",
      "Cost after epoch 520: 2.239502633476303\n",
      "Cost after epoch 530: 2.2296810833793854\n",
      "Cost after epoch 540: 2.2198564992223115\n",
      "Cost after epoch 550: 2.2101103484073343\n",
      "Cost after epoch 560: 2.2006920689417178\n",
      "Cost after epoch 570: 2.191511775126584\n",
      "Cost after epoch 580: 2.1826101673845777\n",
      "Cost after epoch 590: 2.1740378189148153\n",
      "Cost after epoch 600: 2.1658346895188183\n",
      "Cost after epoch 610: 2.1581670420957764\n",
      "Cost after epoch 620: 2.1509149868745987\n",
      "Cost after epoch 630: 2.144070212190219\n",
      "Cost after epoch 640: 2.137632178312303\n",
      "Cost after epoch 650: 2.1315933234541045\n",
      "Cost after epoch 660: 2.126039317951626\n",
      "Cost after epoch 670: 2.1208513261452966\n",
      "Cost after epoch 680: 2.115998542315851\n",
      "Cost after epoch 690: 2.1114602454694404\n",
      "Cost after epoch 700: 2.107214954626612\n",
      "Cost after epoch 710: 2.1033107262006143\n",
      "Cost after epoch 720: 2.0996556197967764\n",
      "Cost after epoch 730: 2.0962224293507883\n",
      "Cost after epoch 740: 2.0929932770642368\n",
      "Cost after epoch 750: 2.089951519422462\n",
      "Cost after epoch 760: 2.0871323063326264\n",
      "Cost after epoch 770: 2.0844710639830604\n",
      "Cost after epoch 780: 2.0819498978206807\n",
      "Cost after epoch 790: 2.0795580234948625\n",
      "Cost after epoch 800: 2.077285768299088\n",
      "Cost after epoch 810: 2.075162686355617\n",
      "Cost after epoch 820: 2.0731433932568764\n",
      "Cost after epoch 830: 2.071216996436142\n",
      "Cost after epoch 840: 2.069377785484797\n",
      "Cost after epoch 850: 2.0676207024702205\n",
      "Cost after epoch 860: 2.065971040138033\n",
      "Cost after epoch 870: 2.0643956764161207\n",
      "Cost after epoch 880: 2.0628878395728174\n",
      "Cost after epoch 890: 2.0614445880829413\n",
      "Cost after epoch 900: 2.06006330949672\n",
      "Cost after epoch 910: 2.058765146143469\n",
      "Cost after epoch 920: 2.057525048554579\n",
      "Cost after epoch 930: 2.0563385500271396\n",
      "Cost after epoch 940: 2.055204093097136\n",
      "Cost after epoch 950: 2.0541202853074\n",
      "Cost after epoch 960: 2.0531042519016625\n",
      "Cost after epoch 970: 2.052136751988067\n",
      "Cost after epoch 980: 2.05121469157118\n",
      "Cost after epoch 990: 2.05033720554921\n",
      "Cost after epoch 1000: 2.04950350609432\n",
      "Cost after epoch 1010: 2.048726899902678\n",
      "Cost after epoch 1020: 2.047992662987356\n",
      "Cost after epoch 1030: 2.0472985455972297\n",
      "Cost after epoch 1040: 2.04664396612056\n",
      "Cost after epoch 1050: 2.0460283559128416\n",
      "Cost after epoch 1060: 2.0454613707392104\n",
      "Cost after epoch 1070: 2.0449319385728217\n",
      "Cost after epoch 1080: 2.0444382822715554\n",
      "Cost after epoch 1090: 2.043979812392561\n",
      "Cost after epoch 1100: 2.043555908112525\n",
      "Cost after epoch 1110: 2.0431727961242667\n",
      "Cost after epoch 1120: 2.042822419499246\n",
      "Cost after epoch 1130: 2.0425032352167505\n",
      "Cost after epoch 1140: 2.042214486884392\n",
      "Cost after epoch 1150: 2.041955370810207\n",
      "Cost after epoch 1160: 2.04172908429607\n",
      "Cost after epoch 1170: 2.0415300960977505\n",
      "Cost after epoch 1180: 2.0413570026178443\n",
      "Cost after epoch 1190: 2.0412088664136436\n",
      "Cost after epoch 1200: 2.041084711768175\n",
      "Cost after epoch 1210: 2.040985295001873\n",
      "Cost after epoch 1220: 2.04090719537816\n",
      "Cost after epoch 1230: 2.040849157269419\n",
      "Cost after epoch 1240: 2.0408101456630945\n",
      "Cost after epoch 1250: 2.0407891084305994\n",
      "Cost after epoch 1260: 2.0407850453926875\n",
      "Cost after epoch 1270: 2.0407962469435534\n",
      "Cost after epoch 1280: 2.0408216513480992\n",
      "Cost after epoch 1290: 2.040860238034729\n",
      "Cost after epoch 1300: 2.040910990682427\n",
      "Cost after epoch 1310: 2.0409718187263217\n",
      "Cost after epoch 1320: 2.041042320164489\n",
      "Cost after epoch 1330: 2.041121662045438\n",
      "Cost after epoch 1340: 2.041208930479503\n",
      "Cost after epoch 1350: 2.041303231324853\n",
      "Cost after epoch 1360: 2.0414019519159163\n",
      "Cost after epoch 1370: 2.0415056043787145\n",
      "Cost after epoch 1380: 2.0416135907451607\n",
      "Cost after epoch 1390: 2.0417251589899394\n",
      "Cost after epoch 1400: 2.0418395854721685\n",
      "Cost after epoch 1410: 2.041954172332771\n",
      "Cost after epoch 1420: 2.0420699898172128\n",
      "Cost after epoch 1430: 2.042186655948097\n",
      "Cost after epoch 1440: 2.042303601800458\n",
      "Cost after epoch 1450: 2.042420290037189\n",
      "Cost after epoch 1460: 2.0425342419028465\n",
      "Cost after epoch 1470: 2.042646790152815\n",
      "Cost after epoch 1480: 2.0427577353859707\n",
      "Cost after epoch 1490: 2.0428666891922633\n",
      "Cost after epoch 1500: 2.0429732943110794\n",
      "Cost after epoch 1510: 2.0430754772214943\n",
      "Cost after epoch 1520: 2.043174609035059\n",
      "Cost after epoch 1530: 2.043270634460771\n",
      "Cost after epoch 1540: 2.04336332834408\n",
      "Cost after epoch 1550: 2.0434524938708347\n",
      "Cost after epoch 1560: 2.0435365501665816\n",
      "Cost after epoch 1570: 2.043616753564496\n",
      "Cost after epoch 1580: 2.043693154604071\n",
      "Cost after epoch 1590: 2.043765665225438\n",
      "Cost after epoch 1600: 2.043834221224529\n",
      "Cost after epoch 1610: 2.0438977432084644\n",
      "Cost after epoch 1620: 2.0439572822990644\n",
      "Cost after epoch 1630: 2.044012958658136\n",
      "Cost after epoch 1640: 2.0440647894530346\n",
      "Cost after epoch 1650: 2.0441128099734747\n",
      "Cost after epoch 1660: 2.044156392854157\n",
      "Cost after epoch 1670: 2.0441963439515827\n",
      "Cost after epoch 1680: 2.0442328194001447\n",
      "Cost after epoch 1690: 2.0442659064252098\n",
      "Cost after epoch 1700: 2.044295703884776\n",
      "Cost after epoch 1710: 2.044321949632939\n",
      "Cost after epoch 1720: 2.044345201206173\n",
      "Cost after epoch 1730: 2.044365620372986\n",
      "Cost after epoch 1740: 2.0443833289328213\n",
      "Cost after epoch 1750: 2.044398453792186\n",
      "Cost after epoch 1760: 2.0444109951478704\n",
      "Cost after epoch 1770: 2.0444212835337714\n",
      "Cost after epoch 1780: 2.0444294609974265\n",
      "Cost after epoch 1790: 2.044435651966437\n",
      "Cost after epoch 1800: 2.0444399802880144\n",
      "Cost after epoch 1810: 2.0444426094230916\n",
      "Cost after epoch 1820: 2.044443677599219\n",
      "Cost after epoch 1830: 2.044443288937943\n",
      "Cost after epoch 1840: 2.044441546026657\n",
      "Cost after epoch 1850: 2.0444385468123887\n",
      "Cost after epoch 1860: 2.0444345384678293\n",
      "Cost after epoch 1870: 2.0444295059820026\n",
      "Cost after epoch 1880: 2.0444235061528335\n",
      "Cost after epoch 1890: 2.044416605537459\n",
      "Cost after epoch 1900: 2.0444088640821056\n",
      "Cost after epoch 1910: 2.044400561834593\n",
      "Cost after epoch 1920: 2.0443915671909396\n",
      "Cost after epoch 1930: 2.0443818893377514\n",
      "Cost after epoch 1940: 2.0443715556668676\n",
      "Cost after epoch 1950: 2.0443605870976134\n",
      "Cost after epoch 1960: 2.0443492761358417\n",
      "Cost after epoch 1970: 2.0443374044837275\n",
      "Cost after epoch 1980: 2.044324941261102\n",
      "Cost after epoch 1990: 2.0443118812340946\n",
      "Cost after epoch 2000: 2.0442982145532937\n",
      "Cost after epoch 2010: 2.0442842511999983\n",
      "Cost after epoch 2020: 2.0442697075840006\n",
      "Cost after epoch 2030: 2.0442545256047278\n",
      "Cost after epoch 2040: 2.044238680553753\n",
      "Cost after epoch 2050: 2.044222145998458\n",
      "Cost after epoch 2060: 2.044205270224546\n",
      "Cost after epoch 2070: 2.0441877177580228\n",
      "Cost after epoch 2080: 2.04416941819045\n",
      "Cost after epoch 2090: 2.0441503430834302\n",
      "Cost after epoch 2100: 2.0441304653703773\n",
      "Cost after epoch 2110: 2.0441101966719515\n",
      "Cost after epoch 2120: 2.0440891570772863\n",
      "Cost after epoch 2130: 2.0440672775067124\n",
      "Cost after epoch 2140: 2.044044539984968\n",
      "Cost after epoch 2150: 2.04402093041491\n",
      "Cost after epoch 2160: 2.043996942199716\n",
      "Cost after epoch 2170: 2.0439721551989805\n",
      "Cost after epoch 2180: 2.0439465112677526\n",
      "Cost after epoch 2190: 2.0439200122002634\n",
      "Cost after epoch 2200: 2.0438926650291345\n",
      "Cost after epoch 2210: 2.04386504938353\n",
      "Cost after epoch 2220: 2.043836708611286\n",
      "Cost after epoch 2230: 2.0438075996212004\n",
      "Cost after epoch 2240: 2.043777746732868\n",
      "Cost after epoch 2250: 2.0437471795487854\n",
      "Cost after epoch 2260: 2.043716551881686\n",
      "Cost after epoch 2270: 2.0436853760887645\n",
      "Cost after epoch 2280: 2.0436536229831552\n",
      "Cost after epoch 2290: 2.043621335983304\n",
      "Cost after epoch 2300: 2.0435885627407298\n",
      "Cost after epoch 2310: 2.0435560051704744\n",
      "Cost after epoch 2320: 2.0435231533758342\n",
      "Cost after epoch 2330: 2.0434899872815566\n",
      "Cost after epoch 2340: 2.0434565617844296\n",
      "Cost after epoch 2350: 2.0434229343362755\n",
      "Cost after epoch 2360: 2.0433898206551877\n",
      "Cost after epoch 2370: 2.0433567007522755\n",
      "Cost after epoch 2380: 2.043323557457165\n",
      "Cost after epoch 2390: 2.0432904479750875\n",
      "Cost after epoch 2400: 2.043257430263948\n",
      "Cost after epoch 2410: 2.043225198233911\n",
      "Cost after epoch 2420: 2.0431932363644965\n",
      "Cost after epoch 2430: 2.0431615247304262\n",
      "Cost after epoch 2440: 2.043130114451584\n",
      "Cost after epoch 2450: 2.0430990558565747\n",
      "Cost after epoch 2460: 2.0430689901148793\n",
      "Cost after epoch 2470: 2.04303942122353\n",
      "Cost after epoch 2480: 2.0430103225611504\n",
      "Cost after epoch 2490: 2.0429817329910764\n",
      "Cost after epoch 2500: 2.0429536895004436\n",
      "Cost after epoch 2510: 2.042926758226864\n",
      "Cost after epoch 2520: 2.0429004760267717\n",
      "Cost after epoch 2530: 2.042874807681136\n",
      "Cost after epoch 2540: 2.042849776348521\n",
      "Cost after epoch 2550: 2.0428254027246733\n",
      "Cost after epoch 2560: 2.042802166005185\n",
      "Cost after epoch 2570: 2.042779645647552\n",
      "Cost after epoch 2580: 2.042757797685524\n",
      "Cost after epoch 2590: 2.0427366286477606\n",
      "Cost after epoch 2600: 2.042716142447561\n",
      "Cost after epoch 2610: 2.042696729714409\n",
      "Cost after epoch 2620: 2.0426780174367174\n",
      "Cost after epoch 2630: 2.0426599539300625\n",
      "Cost after epoch 2640: 2.042642530121639\n",
      "Cost after epoch 2650: 2.042625734494859\n",
      "Cost after epoch 2660: 2.0426098764535334\n",
      "Cost after epoch 2670: 2.0425946302199858\n",
      "Cost after epoch 2680: 2.0425799379970515\n",
      "Cost after epoch 2690: 2.042565777423196\n",
      "Cost after epoch 2700: 2.042552124073754\n",
      "Cost after epoch 2710: 2.0425392204626442\n",
      "Cost after epoch 2720: 2.042526782813412\n",
      "Cost after epoch 2730: 2.0425147489112914\n",
      "Cost after epoch 2740: 2.0425030860970526\n",
      "Cost after epoch 2750: 2.042491760139344\n",
      "Cost after epoch 2760: 2.0424809658189376\n",
      "Cost after epoch 2770: 2.042470451992101\n",
      "Cost after epoch 2780: 2.042460153488898\n",
      "Cost after epoch 2790: 2.042450030548942\n",
      "Cost after epoch 2800: 2.042440042366126\n",
      "Cost after epoch 2810: 2.042430357876792\n",
      "Cost after epoch 2820: 2.0424207461538475\n",
      "Cost after epoch 2830: 2.0424111401339076\n",
      "Cost after epoch 2840: 2.042401496028142\n",
      "Cost after epoch 2850: 2.0423917695120313\n",
      "Cost after epoch 2860: 2.0423821264598248\n",
      "Cost after epoch 2870: 2.0423723418821544\n",
      "Cost after epoch 2880: 2.0423623474339854\n",
      "Cost after epoch 2890: 2.0423520980418775\n",
      "Cost after epoch 2900: 2.042341548557775\n",
      "Cost after epoch 2910: 2.042330883958522\n",
      "Cost after epoch 2920: 2.0423198701895253\n",
      "Cost after epoch 2930: 2.0423084378153153\n",
      "Cost after epoch 2940: 2.0422965427832684\n",
      "Cost after epoch 2950: 2.0422841413601325\n",
      "Cost after epoch 2960: 2.0422714580077073\n",
      "Cost after epoch 2970: 2.0422582348804443\n",
      "Cost after epoch 2980: 2.0422444012964545\n",
      "Cost after epoch 2990: 2.0422299160492714\n",
      "Cost after epoch 3000: 2.0422147385708187\n",
      "Cost after epoch 3010: 2.042199150575904\n",
      "Cost after epoch 3020: 2.042182856794401\n",
      "Cost after epoch 3030: 2.0421657848867842\n",
      "Cost after epoch 3040: 2.04214789784174\n",
      "Cost after epoch 3050: 2.0421291595295092\n",
      "Cost after epoch 3060: 2.0421099235403437\n",
      "Cost after epoch 3070: 2.0420898452495373\n",
      "Cost after epoch 3080: 2.042068850087419\n",
      "Cost after epoch 3090: 2.042046906143041\n",
      "Cost after epoch 3100: 2.0420239825580726\n",
      "Cost after epoch 3110: 2.042000516003734\n",
      "Cost after epoch 3120: 2.0419761026870993\n",
      "Cost after epoch 3130: 2.0419506651606665\n",
      "Cost after epoch 3140: 2.041924177125036\n",
      "Cost after epoch 3150: 2.0418966134391034\n",
      "Cost after epoch 3160: 2.041868501588471\n",
      "Cost after epoch 3170: 2.041839371146587\n",
      "Cost after epoch 3180: 2.0418091411385824\n",
      "Cost after epoch 3190: 2.0417777910526396\n",
      "Cost after epoch 3200: 2.0417453015840126\n",
      "Cost after epoch 3210: 2.041712295410912\n",
      "Cost after epoch 3220: 2.0416782304321788\n",
      "Cost after epoch 3230: 2.041643021552799\n",
      "Cost after epoch 3240: 2.041606653951775\n",
      "Cost after epoch 3250: 2.0415691140164403\n",
      "Cost after epoch 3260: 2.041531120887599\n",
      "Cost after epoch 3270: 2.0414920581217606\n",
      "Cost after epoch 3280: 2.041451836011479\n",
      "Cost after epoch 3290: 2.041410445118795\n",
      "Cost after epoch 3300: 2.041367877177439\n",
      "Cost after epoch 3310: 2.0413249462906577\n",
      "Cost after epoch 3320: 2.0412809612283214\n",
      "Cost after epoch 3330: 2.0412358273084372\n",
      "Cost after epoch 3340: 2.041189540015562\n",
      "Cost after epoch 3350: 2.041142095940684\n",
      "Cost after epoch 3360: 2.0410944002737925\n",
      "Cost after epoch 3370: 2.04104568855599\n",
      "Cost after epoch 3380: 2.040995860915862\n",
      "Cost after epoch 3390: 2.0409449171998384\n",
      "Cost after epoch 3400: 2.0408928582755004\n",
      "Cost after epoch 3410: 2.0408406745896106\n",
      "Cost after epoch 3420: 2.040787531746702\n",
      "Cost after epoch 3430: 2.040733324624396\n",
      "Cost after epoch 3440: 2.0406780568158616\n",
      "Cost after epoch 3450: 2.0406217328377347\n",
      "Cost after epoch 3460: 2.040565421032754\n",
      "Cost after epoch 3470: 2.040508221802476\n",
      "Cost after epoch 3480: 2.0404500248623427\n",
      "Cost after epoch 3490: 2.040390836920547\n",
      "Cost after epoch 3500: 2.0403306655050915\n",
      "Cost after epoch 3510: 2.0402706483694946\n",
      "Cost after epoch 3520: 2.040209826587373\n",
      "Cost after epoch 3530: 2.040148084940911\n",
      "Cost after epoch 3540: 2.040085432635002\n",
      "Cost after epoch 3550: 2.0400218795899634\n",
      "Cost after epoch 3560: 2.0399586237358407\n",
      "Cost after epoch 3570: 2.039894653542231\n",
      "Cost after epoch 3580: 2.039829849211589\n",
      "Cost after epoch 3590: 2.0397642218629755\n",
      "Cost after epoch 3600: 2.039697783229803\n",
      "Cost after epoch 3610: 2.0396317817702556\n",
      "Cost after epoch 3620: 2.0395651606484324\n",
      "Cost after epoch 3630: 2.0394977959461187\n",
      "Cost after epoch 3640: 2.039429700165264\n",
      "Cost after epoch 3650: 2.0393608863273176\n",
      "Cost after epoch 3660: 2.0392926436453127\n",
      "Cost after epoch 3670: 2.0392238776026055\n",
      "Cost after epoch 3680: 2.039154460703874\n",
      "Cost after epoch 3690: 2.0390844063633478\n",
      "Cost after epoch 3700: 2.0390137284280523\n",
      "Cost after epoch 3710: 2.038943747209307\n",
      "Cost after epoch 3720: 2.0388733382430204\n",
      "Cost after epoch 3730: 2.03880237106322\n",
      "Cost after epoch 3740: 2.038730859590863\n",
      "Cost after epoch 3750: 2.0386588181022565\n",
      "Cost after epoch 3760: 2.0385875886619775\n",
      "Cost after epoch 3770: 2.0385160245155607\n",
      "Cost after epoch 3780: 2.0384439928746714\n",
      "Cost after epoch 3790: 2.038371507826929\n",
      "Cost after epoch 3800: 2.038298583747534\n",
      "Cost after epoch 3810: 2.038226575571495\n",
      "Cost after epoch 3820: 2.038154321683326\n",
      "Cost after epoch 3830: 2.0380816876427787\n",
      "Cost after epoch 3840: 2.038008687427012\n",
      "Cost after epoch 3850: 2.037935335242563\n",
      "Cost after epoch 3860: 2.0378629905847747\n",
      "Cost after epoch 3870: 2.0377904840593097\n",
      "Cost after epoch 3880: 2.0377176802489543\n",
      "Cost after epoch 3890: 2.0376445928015796\n",
      "Cost after epoch 3900: 2.0375712355453506\n",
      "Cost after epoch 3910: 2.0374989648737474\n",
      "Cost after epoch 3920: 2.0374266102587804\n",
      "Cost after epoch 3930: 2.0373540359704396\n",
      "Cost after epoch 3940: 2.037281255160854\n",
      "Cost after epoch 3950: 2.03720828112174\n",
      "Cost after epoch 3960: 2.0371364601753617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 3970: 2.0370646268052117\n",
      "Cost after epoch 3980: 2.036992645609124\n",
      "Cost after epoch 3990: 2.0369205291227748\n",
      "Cost after epoch 4000: 2.0368482899882396\n",
      "Cost after epoch 4010: 2.0367772581998533\n",
      "Cost after epoch 4020: 2.036706278857787\n",
      "Cost after epoch 4030: 2.0366352174946445\n",
      "Cost after epoch 4040: 2.036564085947886\n",
      "Cost after epoch 4050: 2.0364928961347477\n",
      "Cost after epoch 4060: 2.036422956177106\n",
      "Cost after epoch 4070: 2.036353126836894\n",
      "Cost after epoch 4080: 2.036283275146934\n",
      "Cost after epoch 4090: 2.0362134121970175\n",
      "Cost after epoch 4100: 2.0361435491356996\n",
      "Cost after epoch 4110: 2.036074967363827\n",
      "Cost after epoch 4120: 2.03600654778602\n",
      "Cost after epoch 4130: 2.035938159453814\n",
      "Cost after epoch 4140: 2.035869812685965\n",
      "Cost after epoch 4150: 2.0358015178436846\n",
      "Cost after epoch 4160: 2.035734525426149\n",
      "Cost after epoch 4170: 2.035667740406289\n",
      "Cost after epoch 4180: 2.0356010343223687\n",
      "Cost after epoch 4190: 2.03553441671921\n",
      "Cost after epoch 4200: 2.035467897171632\n",
      "Cost after epoch 4210: 2.035402691728043\n",
      "Cost after epoch 4220: 2.035337732818735\n",
      "Cost after epoch 4230: 2.0352728948844447\n",
      "Cost after epoch 4240: 2.0352081867086658\n",
      "Cost after epoch 4250: 2.0351436170955144\n",
      "Cost after epoch 4260: 2.0350803646832554\n",
      "Cost after epoch 4270: 2.0350173922379655\n",
      "Cost after epoch 4280: 2.0349545774659568\n",
      "Cost after epoch 4290: 2.034891928413254\n",
      "Cost after epoch 4300: 2.034829453139537\n",
      "Cost after epoch 4310: 2.0347682904583766\n",
      "Cost after epoch 4320: 2.034707435870325\n",
      "Cost after epoch 4330: 2.034646770658542\n",
      "Cost after epoch 4340: 2.034586302163118\n",
      "Cost after epoch 4350: 2.034526037732681\n",
      "Cost after epoch 4360: 2.034467074440332\n",
      "Cost after epoch 4370: 2.034408442474003\n",
      "Cost after epoch 4380: 2.034350026954828\n",
      "Cost after epoch 4390: 2.0342918345532923\n",
      "Cost after epoch 4400: 2.0342338719446564\n",
      "Cost after epoch 4410: 2.0341771929987886\n",
      "Cost after epoch 4420: 2.034120864133583\n",
      "Cost after epoch 4430: 2.0340647745215117\n",
      "Cost after epoch 4440: 2.034008930202268\n",
      "Cost after epoch 4450: 2.033953337217518\n",
      "Cost after epoch 4460: 2.0338990051798036\n",
      "Cost after epoch 4470: 2.033845037904972\n",
      "Cost after epoch 4480: 2.0337913287852944\n",
      "Cost after epoch 4490: 2.033737883269203\n",
      "Cost after epoch 4500: 2.03368470680497\n",
      "Cost after epoch 4510: 2.0336327640601772\n",
      "Cost after epoch 4520: 2.0335811970771247\n",
      "Cost after epoch 4530: 2.0335299035946104\n",
      "Cost after epoch 4540: 2.033478888508778\n",
      "Cost after epoch 4550: 2.0334281567139016\n",
      "Cost after epoch 4560: 2.0333786275718957\n",
      "Cost after epoch 4570: 2.0333294818743073\n",
      "Cost after epoch 4580: 2.033280621794798\n",
      "Cost after epoch 4590: 2.0332320517147466\n",
      "Cost after epoch 4600: 2.033183776012202\n",
      "Cost after epoch 4610: 2.0331366686730132\n",
      "Cost after epoch 4620: 2.0330899494870636\n",
      "Cost after epoch 4630: 2.033043525116255\n",
      "Cost after epoch 4640: 2.0329973994627095\n",
      "Cost after epoch 4650: 2.032951576423898\n",
      "Cost after epoch 4660: 2.032906884795924\n",
      "Cost after epoch 4670: 2.0328625833723595\n",
      "Cost after epoch 4680: 2.032818583325137\n",
      "Cost after epoch 4690: 2.032774888110334\n",
      "Cost after epoch 4700: 2.0327315011781213\n",
      "Cost after epoch 4710: 2.032689206547289\n",
      "Cost after epoch 4720: 2.032647301804473\n",
      "Cost after epoch 4730: 2.032605702625158\n",
      "Cost after epoch 4740: 2.0325644120502093\n",
      "Cost after epoch 4750: 2.0325234331133517\n",
      "Cost after epoch 4760: 2.0324835056668618\n",
      "Cost after epoch 4770: 2.0324439656893873\n",
      "Cost after epoch 4780: 2.0324047333283835\n",
      "Cost after epoch 4790: 2.032365811238021\n",
      "Cost after epoch 4800: 2.032327202064126\n",
      "Cost after epoch 4810: 2.032289602276515\n",
      "Cost after epoch 4820: 2.03225238567776\n",
      "Cost after epoch 4830: 2.0322154768336005\n",
      "Cost after epoch 4840: 2.0321788780379055\n",
      "Cost after epoch 4850: 2.0321425915750186\n",
      "Cost after epoch 4860: 2.032107271466944\n",
      "Cost after epoch 4870: 2.032072328626368\n",
      "Cost after epoch 4880: 2.03203769196435\n",
      "Cost after epoch 4890: 2.0320033634389607\n",
      "Cost after epoch 4900: 2.031969344997617\n",
      "Cost after epoch 4910: 2.0319362492791924\n",
      "Cost after epoch 4920: 2.031903523466419\n",
      "Cost after epoch 4930: 2.031871100726031\n",
      "Cost after epoch 4940: 2.0318389827031935\n",
      "Cost after epoch 4950: 2.0318071710313683\n",
      "Cost after epoch 4960: 2.0317762381423985\n",
      "Cost after epoch 4970: 2.0317456665404108\n",
      "Cost after epoch 4980: 2.03171539354391\n",
      "Cost after epoch 4990: 2.0316854205067183\n",
      "training time: 0:00:02.142011\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loser's's neighbor words: ['game', 'market', \"loser's\", 'a']\n",
      "is's neighbor words: ['game', 'market', \"loser's\", 'a']\n",
      "stock's neighbor words: ['investing', 'a', 'is', 'beating']\n",
      "the's neighbor words: ['is', 'of', 'beating', 'stock']\n",
      "of's neighbor words: ['the', 'of', 'costs', 'beating']\n",
      "deduction's neighbor words: ['costs', 'after', 'the', 'beating']\n",
      "beating's neighbor words: ['market', 'stock', 'costs', 'investing']\n",
      "a's neighbor words: ['game', 'market', \"loser's\", 'a']\n",
      "market's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "game's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "after's neighbor words: ['of', 'the', 'deduction', 'costs']\n",
      "costs's neighbor words: ['beating', 'of', 'deduction', 'the']\n",
      "investing's neighbor words: ['the', 'stock', 'beating', 'costs']\n"
     ]
    }
   ],
   "source": [
    "for input_ind in range(vocab_size):\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- https://github.com/ujhuyz0110/wrd_emb/blob/master/word2vec_skipgram_medium_v1.ipynb\n",
    "-https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72\n",
    "\n",
    "- https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
